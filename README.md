###                                                           Toxic Comment Classification

#### Venkatesh Kandibanda 

### Overview
  
  With the recent growth of people on the internet, civil conversations are seeing a decline. “Whatever intelligent observations do lurk there are often drowned out by obscenities, ad-hominem attacks, and off-topic rants.” These things are forcing many online platforms which once flourished with intellectual discussions to close the comment sections. To facilitate meaningful conversations on their online platform The New York Times employed full-time moderators who moderate nearly 11,000 comments per day on the selected article(roughly 10% of Times articles). However, for small firms operating people for these tasks might be out of scope. To aid, the Conversation AI team, a research initiative founded by Jigsaw and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they’ve built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content)
  
### Data

Source: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview 

- It consists of large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are: a. toxic b. severe_toxic c. obscene d. threat e. insult f. identity_hate

Here is the Detailed Blog link for Toxic Comment Classification https://medium.com/@venkateshkandibanda471/toxic-comment-classification-9c748364883b.
