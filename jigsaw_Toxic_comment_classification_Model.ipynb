{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Toxic Comment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Venkatesh Kandibanda </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Overview</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      With the recent growth of people on the internet, civil conversations are seeing a decline. “Whatever intelligent observations do lurk there are often drowned out by obscenities, ad-hominem attacks, and off-topic rants.” These things are forcing many online platforms which once flourished with intellectual discussions to close the comment sections. To facilitate meaningful conversations on their online platform The New York Times employed full-time moderators who moderate nearly 11,000 comments per day on the selected article(roughly 10% of Times articles). However, for small firms operating people for these tasks might be out of scope. To aid, the Conversation AI team, a research initiative founded by Jigsaw and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they’ve built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don’t allow users to select which types of toxicity they’re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview \n",
    "It consists of large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are: a. toxic \n",
    "              b. severe_toxic \n",
    "              c. obscene \n",
    "              d. threat \n",
    "              e. insult \n",
    "              f. identity_hate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - To Build a Multi-headed model that is capable for detecting different typse of toxicity like threats,obsencity,insult and indentity hate based comments from wikipedia's talk page edits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives and  Constarints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  To effectively facilitate conversations, leading many communities to limit or completely shut down user comments.\n",
    " -  No Low Latency Constarint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - mean column wise ROC AUC Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "x=\"Class1\"+\"23\"\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= pd.read_csv('train.csv/train.csv')\n",
    "test_data= pd.read_csv('test.csv/test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data (159571, 8)\n",
      "shape of test data (153164, 2)\n",
      "columns in train data Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate'],\n",
      "      dtype='object')\n",
      "columns in test data Index(['id', 'comment_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('shape of train data',train_data.shape)\n",
    "print('shape of test data',test_data.shape)\n",
    "print('columns in train data',train_data.columns)\n",
    "print('columns in test data',test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      "id               159571 non-null object\n",
      "comment_text     159571 non-null object\n",
      "toxic            159571 non-null int64\n",
      "severe_toxic     159571 non-null int64\n",
      "obscene          159571 non-null int64\n",
      "threat           159571 non-null int64\n",
      "insult           159571 non-null int64\n",
      "identity_hate    159571 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 153164 entries, 0 to 153163\n",
      "Data columns (total 2 columns):\n",
      "id              153164 non-null object\n",
      "comment_text    153164 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>153164</td>\n",
       "      <td>153164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>153164</td>\n",
       "      <td>153164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>4594229216ddcb31</td>\n",
       "      <td>\" \\n\\n == population density == \\n\\n the artic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text\n",
       "count             153164                                             153164\n",
       "unique            153164                                             153164\n",
       "top     4594229216ddcb31  \" \\n\\n == population density == \\n\\n the artic...\n",
       "freq                   1                                                  1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking for duplicates the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of duplicated data: 0\n"
     ]
    }
   ],
   "source": [
    "print('Numbers of duplicated data:', train_data.duplicated('id').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of duplicated data: 0\n"
     ]
    }
   ],
   "source": [
    "print('Numbers of duplicated data:', test_data.duplicated('id').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments =  159571\n",
      "Total clean comments =  143346\n",
      "Total tags = 35098\n"
     ]
    }
   ],
   "source": [
    "columnsums=train_data.iloc[:,2:].sum()\n",
    "#marking comments without any tags as \"clean\"\n",
    "rowsums=train_data.iloc[:,2:].sum(axis=1)\n",
    "train_data_clean=(rowsums==0)\n",
    "#count number of clean entries\n",
    "train_data_clean.sum()\n",
    "\n",
    "print(\"Total comments = \",len(train_data))\n",
    "print(\"Total clean comments = \",train_data_clean.sum())\n",
    "print(\"Total tags =\",columnsums.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "     >> Here we observe that almost 89.83% of comments are clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_check=train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing any data id               0\n",
      "comment_text     0\n",
      "toxic            0\n",
      "severe_toxic     0\n",
      "obscene          0\n",
      "threat           0\n",
      "insult           0\n",
      "identity_hate    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('missing any data',null_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_check_1=test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing any data id              0\n",
      "comment_text    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('missing any data',null_check_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping the rows with are not required that is id and comment_text\n",
    "data_toxic=train_data.drop(['id','comment_text'],axis=1)\n",
    "data_toxic.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the counts of toxic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names= ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxic</td>\n",
       "      <td>15294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe_toxic</td>\n",
       "      <td>1595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obscene</td>\n",
       "      <td>8449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threat</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insult</td>\n",
       "      <td>7877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category  count\n",
       "0          toxic  15294\n",
       "1   severe_toxic   1595\n",
       "2        obscene   8449\n",
       "3         threat    478\n",
       "4         insult   7877\n",
       "5  identity_hate   1405"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = []\n",
    "categories = list(data_toxic.columns.values)\n",
    "for i in categories:\n",
    "    counts.append((i, data_toxic[i].sum()))\n",
    "data_stats = pd.DataFrame(counts, columns=['category', 'count'])\n",
    "data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://www.kaggle.com/manishachakraborty/toxic-comment-classification\n",
    "import seaborn as sns\n",
    "x=train_data.iloc[:,2:].sum()\n",
    "#plot\n",
    "plt.figure(figsize=(8,4))\n",
    "ax= sns.barplot(x.index, x.values, alpha=0.7)\n",
    "plt.title(\"# per class\")\n",
    "plt.ylabel('# of Occurrences', fontsize=12)\n",
    "plt.xlabel('Type ', fontsize=12)\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# observation:\n",
    "     >> From the above plot it is clear that there are more number of toxic, obscene and insult comments compared to others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtml(sentence):\n",
    "    cleanr=re.compile('<.*?>')\n",
    "    cleantext=re.sub(cleanr, ' ',str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "def cleanPunc(sentence):\n",
    "    cleaned=re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned=cleaned.strip()\n",
    "    cleaned=cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent=\"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word=re.sub('[^a-z A-Z]+',' ',word)\n",
    "        alpha_sent+=alpha_word\n",
    "        alpha_sent+=\" \"\n",
    "    alpha_sent=alpha_sent.strip()\n",
    "    return alpha_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['comment_text']=train_data['comment_text'].str.lower()\n",
    "train_data['comment_text']=train_data['comment_text'].apply(cleanHtml)\n",
    "train_data['comment_text']=train_data['comment_text'].apply(cleanPunc)\n",
    "train_data['comment_text']=train_data['comment_text'].apply(keepAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww he matches this background colour im seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man im really not trying to edit war its j...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>more i cant make any real suggestions on impro...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  explanation why the edits made under my userna...      0   \n",
       "1  000103f0d9cfb60f  daww he matches this background colour im seem...      0   \n",
       "2  000113f07ec002fd  hey man im really not trying to edit war its j...      0   \n",
       "3  0001b41b1c6bb37e  more i cant make any real suggestions on impro...      0   \n",
       "4  0001d958c54c6e35  you sir are my hero any chance you remember wh...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation   edits made   username hardcore m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww  matches  background colour im seemingly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man im really  trying  edit war     guy  c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>cant make  real suggestions  improvement    ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir   hero  chance  remember  page thats on</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  explanation   edits made   username hardcore m...      0   \n",
       "1  000103f0d9cfb60f  daww  matches  background colour im seemingly ...      0   \n",
       "2  000113f07ec002fd  hey man im really  trying  edit war     guy  c...      0   \n",
       "3  0001b41b1c6bb37e    cant make  real suggestions  improvement    ...      0   \n",
       "4  0001d958c54c6e35        sir   hero  chance  remember  page thats on      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop=set(stopwords.words('english'))\n",
    "stop.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','again','beside','yet','within','however'])\n",
    "re_stop=re.compile(r\"\\b(\" + \"|\".join(stop) + \")\\\\W\",re.I)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop\n",
    "    return re_stop.sub(\" \",sentence)\n",
    "\n",
    "train_data['comment_text']=train_data['comment_text'].apply(removeStopWords)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explan edit made usernam hardcor metallica fan...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww match background colour im seem stuck tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man im realli tri edit war guy constant re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>cant make real suggest improv wonder section s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chanc rememb page that on</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  explan edit made usernam hardcor metallica fan...      0   \n",
       "1  000103f0d9cfb60f  daww match background colour im seem stuck tha...      0   \n",
       "2  000113f07ec002fd  hey man im realli tri edit war guy constant re...      0   \n",
       "3  0001b41b1c6bb37e  cant make real suggest improv wonder section s...      0   \n",
       "4  0001d958c54c6e35                 sir hero chanc rememb page that on      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "train_data['comment_text'] = train_data['comment_text'].apply(stemming)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text'], dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['comment_text']=test_data['comment_text'].str.lower()\n",
    "test_data['comment_text']=test_data['comment_text'].apply(cleanHtml)\n",
    "test_data['comment_text']=test_data['comment_text'].apply(cleanPunc)\n",
    "test_data['comment_text']=test_data['comment_text'].apply(keepAlpha)\n",
    "test_data['comment_text']=test_data['comment_text'].apply(removeStopWords)\n",
    "test_data['comment_text'] = test_data['comment_text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>yo bitch ja rule succes youll ever what hate s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>rfc titl fine imo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>sourc zaw ashton lapland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>look back sourc inform updat correct form gues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>dont anonym edit articl all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  yo bitch ja rule succes youll ever what hate s...\n",
       "1  0000247867823ef7                                  rfc titl fine imo\n",
       "2  00013b17ad220c46                           sourc zaw ashton lapland\n",
       "3  00017563c3f7919a  look back sourc inform updat correct form gues...\n",
       "4  00017695ad8997eb                        dont anonym edit articl all"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test= test_data.drop(['id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_text'], dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yo bitch ja rule succes youll ever what hate s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rfc titl fine imo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sourc zaw ashton lapland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>look back sourc inform updat correct form gues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dont anonym edit articl all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text\n",
       "0  yo bitch ja rule succes youll ever what hate s...\n",
       "1                                  rfc titl fine imo\n",
       "2                           sourc zaw ashton lapland\n",
       "3  look back sourc inform updat correct form gues...\n",
       "4                        dont anonym edit articl all"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_total=train_data['comment_text']\n",
    "test_text_total=test_data['comment_text']\n",
    "all_text_total = pd.concat([train_text_total, test_text_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explan edit made usernam hardcor metallica fan...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww match background colour im seem stuck tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  explan edit made usernam hardcor metallica fan...      0   \n",
       "1  000103f0d9cfb60f  daww match background colour im seem stuck tha...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135635, 8)\n",
      "(23936, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train,test=train_test_split(train_data, random_state=42, test_size=0.15, shuffle=True)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text=train['comment_text']\n",
    "test_text=test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135635,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23936,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing the data with Tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<23936x30000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 602286 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.pipeline import make_union\n",
    "word_vectorizer = TfidfVectorizer(sublinear_tf=True,strip_accents='unicode',analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 1),max_features=30000)\n",
    "word_vectorizer.fit(train_text)\n",
    "word_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "dump(word_vectorizer,open('vectorizer_tfidf.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=word_vectorizer.transform(train_text)\n",
    "y_train=train.drop(labels=['id','comment_text'],axis=1)\n",
    "\n",
    "x_test=word_vectorizer.transform(test_text)\n",
    "y_test=test.drop(labels=['id','comment_text'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the x_train (135635, 30000)\n",
      "shape of the x_test (23936, 30000)\n"
     ]
    }
   ],
   "source": [
    "print('shape of the x_train',x_train.shape)\n",
    "print('shape of the x_test',x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135635, 6)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
       "       'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23936, 6)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_data_total (159571, 30000)\n"
     ]
    }
   ],
   "source": [
    "train_data_total=word_vectorizer.transform(train_text_total)\n",
    "print(\"shape of train_data_total\",train_data_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 6)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_total=train_data.drop(labels=['id','comment_text'],axis=1)\n",
    "y_train_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of test_data_total (153164, 30000)\n"
     ]
    }
   ],
   "source": [
    "test_data_total=word_vectorizer.transform(test_text_total)\n",
    "print(\"shape of test_data_total\",test_data_total.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling on Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:  OneVsRestClassifier(estimator=LogisticRegression(C=1, class_weight='balanced',\n",
      "                                                 dual=False, fit_intercept=True,\n",
      "                                                 intercept_scaling=1,\n",
      "                                                 l1_ratio=None, max_iter=100,\n",
      "                                                 multi_class='auto',\n",
      "                                                 n_jobs=None, penalty='l2',\n",
      "                                                 random_state=None,\n",
      "                                                 solver='lbfgs', tol=0.0001,\n",
      "                                                 verbose=0, warm_start=False),\n",
      "                    n_jobs=-1)\n",
      "Best Cross Validation Score:  0.980914764563676\n"
     ]
    }
   ],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html\n",
    "#https://medium.com/@saugata.paul1010/a-detailed-case-study-on-multi-label-classification-with-machine-learning-algorithms-and-72031742c9aa\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from scipy import stats\n",
    "\n",
    "alpha = [0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\n",
    "penalty=['l1','l2']\n",
    "params  = {\"estimator__C\":alpha,\n",
    "           \"estimator__penalty\":penalty}\n",
    "base_estimator = OneVsRestClassifier(LogisticRegression(class_weight='balanced'), n_jobs=-1)\n",
    "rsearch_cv = RandomizedSearchCV(estimator=base_estimator, param_distributions=params, n_iter=10, cv=5, scoring='roc_auc', n_jobs=-1, verbose=0)\n",
    "rsearch_cv.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best estimator: \",rsearch_cv.best_estimator_)\n",
    "print(\"Best Cross Validation Score: \",rsearch_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9712958674938319\n",
      "CV score for class severe_toxic is 0.9869779931014886\n",
      "CV score for class obscene is 0.9869564357541316\n",
      "CV score for class threat is 0.9815785065612058\n",
      "CV score for class insult is 0.977632570422355\n",
      "CV score for class identity_hate is 0.9786239893639861\n",
      "Total CV score is 0.9805108937828332\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = []\n",
    "results=pd.DataFrame()\n",
    "submission = pd.DataFrame.from_dict({'id': test_data['id']})\n",
    "current_time=\n",
    "for class_name in class_names:\n",
    "    train_target = y_train[class_name]\n",
    "    \n",
    "    classifier = OneVsRestClassifier(LogisticRegression(C=1,solver='lbfgs',penalty='l2'))\n",
    "    classifier.fit(x_train, train_target)\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier, x_train, train_target, cv=3, scoring='roc_auc'))\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "    \n",
    "    prediction_log=classifier.predict_proba(x_test)[:, 1]\n",
    "    results[class_name]=(prediction_log)\n",
    "    \n",
    "    submission[class_name] = classifier.predict_proba(test_data_total)[:,1]\n",
    "print('Total CV score is {}'.format(np.mean(scores)))\n",
    "submission.to_csv('submission_test_lr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.161918</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.036917</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.041304</td>\n",
       "      <td>0.008880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007625</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>0.001179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.112042</td>\n",
       "      <td>0.009075</td>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.081120</td>\n",
       "      <td>0.013304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.001172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      toxic  severe_toxic   obscene    threat    insult  identity_hate\n",
       "0  0.161918      0.006732  0.036917  0.002251  0.041304       0.008880\n",
       "1  0.007625      0.001158  0.001874  0.001115  0.003141       0.001179\n",
       "2  0.112042      0.009075  0.040564  0.003200  0.081120       0.013304\n",
       "3  0.002309      0.001433  0.003440  0.000741  0.002389       0.001172"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.997398</td>\n",
       "      <td>0.163815</td>\n",
       "      <td>0.991399</td>\n",
       "      <td>0.039684</td>\n",
       "      <td>0.936991</td>\n",
       "      <td>0.213891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.009255</td>\n",
       "      <td>0.003804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.035225</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.013490</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.016080</td>\n",
       "      <td>0.003220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.003172</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>0.000887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.020858</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.006694</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.007805</td>\n",
       "      <td>0.001873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.997398      0.163815  0.991399  0.039684  0.936991   \n",
       "1  0000247867823ef7  0.010363      0.003122  0.006035  0.001771  0.009255   \n",
       "2  00013b17ad220c46  0.035225      0.003900  0.013490  0.001464  0.016080   \n",
       "3  00017563c3f7919a  0.003172      0.002207  0.002697  0.000960  0.003536   \n",
       "4  00017695ad8997eb  0.020858      0.001391  0.006694  0.001999  0.007805   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.213891  \n",
       "1       0.003804  \n",
       "2       0.003220  \n",
       "3       0.000887  \n",
       "4       0.001873  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_final_1=pd.read_csv('submission_test_lr.csv')\n",
    "sample_final_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on total test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:  OneVsRestClassifier(estimator=LogisticRegression(C=1, class_weight='balanced'),\n",
      "                    n_jobs=-1)\n",
      "Best Cross Validation Score:  0.9813336960945588\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from scipy import stats\n",
    "\n",
    "alpha = [0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\n",
    "penalty=['l1','l2']\n",
    "params  = {\"estimator__C\":alpha,\n",
    "           \"estimator__penalty\":penalty}\n",
    "base_estimator_total = OneVsRestClassifier(LogisticRegression(class_weight='balanced'), n_jobs=-1)\n",
    "rsearch_cv_total = RandomizedSearchCV(estimator=base_estimator_total, param_distributions=params, n_iter=10, cv=5, scoring='roc_auc', n_jobs=-1, verbose=0)\n",
    "rsearch_cv_total.fit(train_data_total, y_train_total)\n",
    "\n",
    "print(\"Best estimator: \",rsearch_cv_total.best_estimator_)\n",
    "print(\"Best Cross Validation Score: \",rsearch_cv_total.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.972456845954271\n",
      "CV score for class severe_toxic is 0.986701387539147\n",
      "CV score for class obscene is 0.9874368036418085\n",
      "CV score for class threat is 0.9830126008236074\n",
      "CV score for class insult is 0.97880652715562\n",
      "CV score for class identity_hate is 0.9777262777616788\n",
      "Total CV score is 0.9810234071460221\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = []\n",
    "results=pd.DataFrame()\n",
    "submission = pd.DataFrame.from_dict({'id': test_data['id']})\n",
    "for class_name in class_names:\n",
    "    file_name=\"LogisticRegression_\"+class_name+\".pkl\"\n",
    "    train_target = train_data[class_name]\n",
    "    \n",
    "    classifier_total = OneVsRestClassifier(LogisticRegression(C=1,solver='lbfgs',penalty='l2',max_iter = 1000))\n",
    "    classifier_total.fit(train_data_total, train_target)\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier_total, train_data_total, train_target, cv=3, scoring='roc_auc'))\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "    \n",
    "    prediction_log_total=classifier_total.predict_proba(test_data_total)[:, 1]\n",
    "    results[class_name]=(prediction_log_total)\n",
    "    dump(classifier_total,open(file_name,'wb'))\n",
    "    submission[class_name] = classifier_total.predict_proba(test_data_total)[:,1]\n",
    "print('Total CV score is {}'.format(np.mean(scores)))\n",
    "submission.to_csv('submission_test_lr_total.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997984</td>\n",
       "      <td>0.160883</td>\n",
       "      <td>0.994528</td>\n",
       "      <td>0.038573</td>\n",
       "      <td>0.944541</td>\n",
       "      <td>0.243099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007465</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>0.005036</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.009193</td>\n",
       "      <td>0.003483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.032511</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>0.012430</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.014662</td>\n",
       "      <td>0.003079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.003582</td>\n",
       "      <td>0.000805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.027779</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.007622</td>\n",
       "      <td>0.001824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      toxic  severe_toxic   obscene    threat    insult  identity_hate\n",
       "0  0.997984      0.160883  0.994528  0.038573  0.944541       0.243099\n",
       "1  0.007465      0.003214  0.005036  0.001603  0.009193       0.003483\n",
       "2  0.032511      0.003813  0.012430  0.001356  0.014662       0.003079\n",
       "3  0.002945      0.002096  0.002652  0.000818  0.003582       0.000805\n",
       "4  0.027779      0.001829  0.006820  0.001841  0.007622       0.001824"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes modelling on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:  OneVsRestClassifier(estimator=MultinomialNB(alpha=0.1, class_prior=None,\n",
      "                                            fit_prior=True),\n",
      "                    n_jobs=None)\n",
      "Best Cross Validation Score:  0.9525336414826123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy import stats\n",
    "\n",
    "alpha = [0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\n",
    "\n",
    "params  = {\"estimator__alpha\":alpha}\n",
    "base_estimator_1 = OneVsRestClassifier(MultinomialNB(alpha=params, class_prior=None, fit_prior=True))\n",
    "rsearch_cv_1 = RandomizedSearchCV(estimator=base_estimator_1, param_distributions=params, n_iter=10, cv=5, scoring='roc_auc', n_jobs=-1, verbose=0)\n",
    "rsearch_cv_1.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best estimator: \",rsearch_cv_1.best_estimator_)\n",
    "print(\"Best Cross Validation Score: \",rsearch_cv_1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9506013565902179\n",
      "CV score for class severe_toxic is 0.9679130968762545\n",
      "CV score for class obscene is 0.9587730887714053\n",
      "CV score for class threat is 0.9203702487798884\n",
      "CV score for class insult is 0.956933578244768\n",
      "CV score for class identity_hate is 0.942268182732366\n",
      "Total CV score is 0.9494765919991499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "scores = []\n",
    "results=pd.DataFrame()\n",
    "submission = pd.DataFrame.from_dict({'id': test_data['id']})\n",
    "for class_name in class_names:\n",
    "    train_target = y_train[class_name]\n",
    "    \n",
    "    classifier_nb = OneVsRestClassifier(MultinomialNB(alpha=0.1))\n",
    "    classifier_nb.fit(x_train,train_target)\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier_nb, x_train, train_target, cv=3, scoring='roc_auc'))\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "    prediction_nb=classifier_nb.predict_proba(x_test)[:, 1]\n",
    "    results[class_name]=(prediction_nb)\n",
    "    \n",
    "    submission[class_name] = classifier_nb.predict_proba(test_data_total)[:,1]\n",
    "    \n",
    "print('Total CV score is {}'.format(np.mean(scores)))\n",
    "submission.to_csv('submission_test_nb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125010</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.033004</td>\n",
       "      <td>2.223076e-04</td>\n",
       "      <td>0.020556</td>\n",
       "      <td>0.004363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005653</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>4.429256e-06</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.251955</td>\n",
       "      <td>0.004377</td>\n",
       "      <td>0.135066</td>\n",
       "      <td>1.171717e-03</td>\n",
       "      <td>0.149314</td>\n",
       "      <td>0.013331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>9.402275e-07</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005533</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>1.914112e-06</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      toxic  severe_toxic   obscene        threat    insult  identity_hate\n",
       "0  0.125010      0.001284  0.033004  2.223076e-04  0.020556       0.004363\n",
       "1  0.005653      0.000031  0.000949  4.429256e-06  0.001250       0.000037\n",
       "2  0.251955      0.004377  0.135066  1.171717e-03  0.149314       0.013331\n",
       "3  0.000168      0.000007  0.000142  9.402275e-07  0.000104       0.000009\n",
       "4  0.005533      0.000017  0.001863  1.914112e-06  0.000880       0.000051"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Logistic Regression with SGD classifier on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:  OneVsRestClassifier(estimator=SGDClassifier(alpha=1e-05, average=False,\n",
      "                                            class_weight=None,\n",
      "                                            early_stopping=False, epsilon=0.1,\n",
      "                                            eta0=0.0, fit_intercept=True,\n",
      "                                            l1_ratio=0.15,\n",
      "                                            learning_rate='optimal', loss='log',\n",
      "                                            max_iter=1000, n_iter_no_change=5,\n",
      "                                            n_jobs=None, penalty='l2',\n",
      "                                            power_t=0.5, random_state=None,\n",
      "                                            shuffle=True, tol=0.001,\n",
      "                                            validation_fraction=0.1, verbose=0,\n",
      "                                            warm_start=False),\n",
      "                    n_jobs=None)\n",
      "Best Cross Validation Score:  0.9809687189770602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy import stats\n",
    "\n",
    "alpha = [0.000001,0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\n",
    "penalty=['l1','l2']\n",
    "params  = {\"estimator__alpha\":alpha,\n",
    "           \"estimator__penalty\":penalty}\n",
    "base_estimator_2 = OneVsRestClassifier(SGDClassifier(loss='log'))\n",
    "rsearch_cv_2 = RandomizedSearchCV(estimator=base_estimator_2, param_distributions=params, n_iter=10, cv=5, scoring='roc_auc', n_jobs=-1, verbose=0)\n",
    "rsearch_cv_2.fit(x_train,y_train)\n",
    "\n",
    "print(\"Best estimator: \",rsearch_cv_2.best_estimator_)\n",
    "print(\"Best Cross Validation Score: \",rsearch_cv_2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9715124347527709\n",
      "CV score for class severe_toxic is 0.9868994174225678\n",
      "CV score for class obscene is 0.9870182927243109\n",
      "CV score for class threat is 0.9818748210265063\n",
      "CV score for class insult is 0.9775883326753653\n",
      "CV score for class identity_hate is 0.9787248486533655\n",
      "Total CV score is 0.9806030245424809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "scores = []\n",
    "results=pd.DataFrame()\n",
    "submission = pd.DataFrame.from_dict({'id': test_data['id']})\n",
    "for class_name in class_names:\n",
    "    train_target = y_train[class_name]\n",
    "    \n",
    "    classifier_lr = OneVsRestClassifier(SGDClassifier(alpha=0.00001,loss='log',penalty='l2'))\n",
    "    classifier_lr.fit(x_train, train_target)\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(\n",
    "        classifier_lr, x_train, train_target, cv=3, scoring='roc_auc'))\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "    \n",
    "    prediction_lr=classifier_lr.predict_proba(x_test)[:, 1]\n",
    "    results[class_name]=(prediction_lr)\n",
    "    \n",
    "    submission[class_name] = classifier_lr.predict_proba(test_data_total)[:,1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(scores)))\n",
    "submission.to_csv('submission_test_sgdlr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.174155</td>\n",
       "      <td>0.006648</td>\n",
       "      <td>0.042081</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.042394</td>\n",
       "      <td>0.009187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009338</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>0.001472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.129669</td>\n",
       "      <td>0.009606</td>\n",
       "      <td>0.045897</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>0.080596</td>\n",
       "      <td>0.013923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.004206</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.001518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008791</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>0.005845</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.002431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      toxic  severe_toxic   obscene    threat    insult  identity_hate\n",
       "0  0.174155      0.006648  0.042081  0.002547  0.042394       0.009187\n",
       "1  0.009338      0.001346  0.002396  0.001357  0.003417       0.001472\n",
       "2  0.129669      0.009606  0.045897  0.003671  0.080596       0.013923\n",
       "3  0.003385      0.001587  0.004206  0.000941  0.002926       0.001518\n",
       "4  0.008791      0.001916  0.005845  0.000957  0.003799       0.002431"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nueral Network Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model,load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate,add,Conv1D\n",
    "from keras.layers import GRU,LSTM, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback,EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import os, re, csv, math, codecs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train_1 (143613, 8)\n",
      "shape of y_train_1 (143613, 6)\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/tunguz/bi-gru-cnn-poolings-gpu-kernel-version\n",
    "train = pd.read_csv('train.csv/train.csv')\n",
    "test = pd.read_csv(\"test.csv/test.csv\")\n",
    "submission = pd.read_csv(\"sample_submission.csv/sample_submission.csv\")\n",
    "EMBEDDING_FILE = 'glove.840B.300d/glove.840B.300d.txt'\n",
    "                \n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "train[\"comment_text\"].fillna(\"no comment\")\n",
    "test[\"comment_text\"].fillna(\"no comment\")\n",
    "X_train_1, X_valid_1, y_train_1, y_valid_1 = train_test_split(train, y, test_size = 0.1)\n",
    "print(\"shape of X_train_1\",X_train_1.shape)\n",
    "print(\"shape of y_train_1\",y_train_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 130000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 220 # max number of words in a comment to use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_train = X_train_1[\"comment_text\"].str.lower()\n",
    "raw_text_valid = X_valid_1[\"comment_text\"].str.lower()\n",
    "raw_text_test = test[\"comment_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(num_words = max_features, lower = True)\n",
    "tok.fit_on_texts(raw_text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "dump(tok,open('tokenizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1[\"comment_seq\"] = tok.texts_to_sequences(raw_text_train)\n",
    "X_valid_1[\"comment_seq\"] = tok.texts_to_sequences(raw_text_valid)\n",
    "test[\"comment_seq\"] = tok.texts_to_sequences(raw_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = pad_sequences(X_train_1.comment_seq, maxlen = maxlen)\n",
    "X_valid_1 = pad_sequences(X_valid_1.comment_seq, maxlen = maxlen)\n",
    "test = pad_sequences(test.comment_seq, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316b9fd310f2403493e06fffac06c079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "count = 0\n",
    "embeddings_index = {}\n",
    "#Have downloaded pretrained embeddings. So loading it here\n",
    "f = codecs.open(EMBEDDING_FILE, encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs.reshape(-1)\n",
    "    coef = embeddings_index[word]\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found %d word vectors of glove. 2195895\n",
      "-0.01444638 0.47249147\n",
      "Total %s word vectors. 2195895\n"
     ]
    }
   ],
   "source": [
    "print('Found %d word vectors of glove.',len(embeddings_index))\n",
    "emb_mean,emb_std = coef.mean(), coef.std()\n",
    "print(emb_mean,emb_std)\n",
    "\n",
    "print('Total %s word vectors.',len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Found 197607 unique tokens\n",
      "Null word embeddings:  52258\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "word_index = tok.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Null word embeddings: ',np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional GRU Model with Glove embedding of 300 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_123 = \"best_model_123.hdf5\"\n",
    "check_point_123 = ModelCheckpoint(file_path_123, monitor = \"val_loss\", verbose = 1,save_best_only = True, mode = \"min\")\n",
    "ra_val = RocAucEvaluation(validation_data=(X_valid_1, y_valid_1), interval = 1)\n",
    "early_stop_123 = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n",
    "    inp = Input(shape = (maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(dr)(x)\n",
    "    x = Bidirectional(GRU(units, return_sequences = True))(x1)\n",
    "    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "\n",
    "    y = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
    "    y = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "\n",
    "    avg_pool1 = GlobalAveragePooling1D()(x)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    avg_pool2 = GlobalAveragePooling1D()(y)\n",
    "    max_pool2 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "    x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "    x = Dense(6, activation = \"sigmoid\")(x)\n",
    "\n",
    "    model_123 = Model(inputs = inp, outputs = x)\n",
    "    model_123.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    model_123 = model_123.fit(X_train_1, y_train_1, batch_size = 128, epochs = 3, validation_data = (X_valid_1, y_valid_1),\n",
    "                        verbose = 1, callbacks = [ra_val, check_point_123, early_stop_123])\n",
    "    model_123 = load_model(file_path_123)\n",
    "    return model_123\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_123 = 0\n",
    "n_seeds = 10\n",
    "\n",
    "for i in range(n_seeds):\n",
    "    model_123 = build_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\n",
    "    pred_123 += model_123.predict(test, batch_size = 1024, verbose = 0)/n_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[list_classes] = (pred_123)\n",
    "submission.to_csv(\"submission_123.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model_123,open('gloveembedding.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE_1 = 'glove.6B/glove.6B.50d.txt'\n",
    "embed_size_1 = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4128055c724d0f842949abd7806f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "embeddings_index_1 = {}\n",
    "#Have downloaded fastext pretrained embeddings. So loading it here\n",
    "f = codecs.open(EMBEDDING_FILE_1, encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.array([float(val) for val in values[1:]])\n",
    "    embeddings_index_1[word] = coefs.reshape(-1)\n",
    "    coef = embeddings_index_1[word]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found %d word vectors of glove. 400000\n",
      "-0.08733540000000001 0.4519085941066844\n",
      "Total %s word vectors. 400000\n"
     ]
    }
   ],
   "source": [
    "print('Found %d word vectors of glove.',len(embeddings_index_1))\n",
    "emb_mean,emb_std = coef.mean(), coef.std()\n",
    "print(emb_mean,emb_std)\n",
    "\n",
    "print('Total %s word vectors.',len(embeddings_index_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Found 197607 unique tokens\n",
      "Null word embeddings:  56561\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "word_index = tok.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "nb_words_1 = min(max_features, len(word_index))\n",
    "embedding_matrix_1 = np.zeros((nb_words_1, embed_size_1))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector_1 = embeddings_index_1.get(word)\n",
    "    if embedding_vector_1 is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix_1[i] = embedding_vector_1\n",
    "\n",
    "print('Null word embeddings: ',np.sum(np.sum(embedding_matrix_1, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional GRU model with Glove Embedding of 50 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 220)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 220, 50)      6500000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 220, 50)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 220, 224)     109536      spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 220, 224)     146048      spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 219, 56)      25144       bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 219, 56)      25144       bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 56)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 56)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 56)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 56)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 224)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            1350        concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,807,222\n",
      "Trainable params: 6,807,222\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inp_1 = Input(shape=(maxlen, ))\n",
    "x = Embedding(nb_words_1, embed_size_1, weights=[embedding_matrix_1])(inp_1)\n",
    "x1 = SpatialDropout1D(0.2)(x)\n",
    "x = Bidirectional(GRU(112, return_sequences=True))(x1)\n",
    "x = Conv1D(56, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "y = Bidirectional(LSTM(112, return_sequences = True))(x1)\n",
    "y = Conv1D(56, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "avg_pool2 = GlobalAveragePooling1D()(y)\n",
    "max_pool2 = GlobalMaxPooling1D()(y)\n",
    "conc = concatenate([avg_pool, max_pool,avg_pool2, max_pool2])\n",
    "outp_1 = Dense(6, activation=\"sigmoid\")(conc)\n",
    "model_1 = Model(inputs=[inp_1], outputs=[outp_1]) \n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = 0.001, decay = 0), metrics = [\"accuracy\"]) \n",
    "file_path_1 = \"best_model_1.hdf5\"\n",
    "check_point_1 = ModelCheckpoint(file_path_1, monitor = \"val_loss\", verbose = 1,\n",
    "                              save_best_only = True, mode = \"min\")\n",
    "ra_val = RocAucEvaluation(validation_data=(X_valid_1, y_valid_1), interval = 1)\n",
    "early_stop_1 = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the model\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 773s 5ms/step - loss: 0.0646 - accuracy: 0.9775 - val_loss: 0.0448 - val_accuracy: 0.9831\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.981961 \n",
      "\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04484, saving model to best_model_1.hdf5\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 768s 5ms/step - loss: 0.0465 - accuracy: 0.9824 - val_loss: 0.0433 - val_accuracy: 0.9835\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.986740 \n",
      "\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04484 to 0.04326, saving model to best_model_1.hdf5\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 770s 5ms/step - loss: 0.0409 - accuracy: 0.9840 - val_loss: 0.0418 - val_accuracy: 0.9836\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.988801 \n",
      "\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04326 to 0.04181, saving model to best_model_1.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1dd05c18ef0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('training the model')\n",
    "model_1.fit(X_train_1, y_train_1, batch_size=128, epochs=3, validation_data=(X_valid_1, y_valid_1),\n",
    "                 callbacks=[ra_val,check_point_1,early_stop_1], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 30s 199us/step\n"
     ]
    }
   ],
   "source": [
    "model_1 = load_model(file_path_1)\n",
    "y_pred_1 = model_1.predict(test, batch_size=1024,verbose=1)\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred_1\n",
    "submission.to_csv('submission_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE_2 = 'fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "LSTM_UNITS =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bd750a733b46ae81c38c420f9178c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "count = 0\n",
    "embeddings_index_2 = {}\n",
    "#Have downloaded fastext pretrained embeddings. So loading it here\n",
    "f = codecs.open(EMBEDDING_FILE_2, encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = ' '.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index_2[word] = coefs.reshape(-1)\n",
    "    coef = embeddings_index_2[word]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found %d word vectors of glove. 1999996\n",
      "0.038247995 0.28484675\n",
      "Total %s word vectors. 1999996\n"
     ]
    }
   ],
   "source": [
    "print('Found %d word vectors of glove.',len(embeddings_index_2))\n",
    "emb_mean,emb_std = coef.mean(), coef.std()\n",
    "print(emb_mean,emb_std)\n",
    "\n",
    "print('Total %s word vectors.',len(embeddings_index_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Found 197607 unique tokens\n",
      "Null word embeddings:  53811\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "word_index = tok.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "nb_words_3 = min(max_features, len(word_index))\n",
    "embedding_matrix_2 = np.zeros((nb_words_3, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector_2 = embeddings_index_2.get(word)\n",
    "    if embedding_vector_2 is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix_2[i] = embedding_vector_2\n",
    "\n",
    "print('Null word embeddings: ',np.sum(np.sum(embedding_matrix_2, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130000, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional GRU with fasttext embedding of 300 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_12 = \"best_model_12.hdf5\"\n",
    "check_point_12 = ModelCheckpoint(file_path_12, monitor = \"val_loss\", verbose = 1,save_best_only = True, mode = \"min\")\n",
    "ra_val = RocAucEvaluation(validation_data=(X_valid_1, y_valid_1), interval = 1)\n",
    "early_stop_12 = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_1(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n",
    "    inp = Input(shape = (maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix_2], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(dr)(x)\n",
    "    x = Bidirectional(GRU(units, return_sequences = True))(x1)\n",
    "    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "\n",
    "    y = Bidirectional(GRU(units, return_sequences = True))(x1)\n",
    "    y = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "\n",
    "    avg_pool1 = GlobalAveragePooling1D()(x)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    avg_pool2 = GlobalAveragePooling1D()(y)\n",
    "    max_pool2 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "    x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "    x = Dense(6, activation = \"sigmoid\")(x)\n",
    "\n",
    "    model_12 = Model(inputs = inp, outputs = x)\n",
    "    model_12.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history_fasttext = model_12.fit(X_train_1, y_train_1, batch_size = 128, epochs = 3, validation_data = (X_valid_1, y_valid_1),\n",
    "                        verbose = 1, callbacks = [ra_val, check_point_12, early_stop_12])\n",
    "    model_12 = load_model(file_path_12)\n",
    "    return model_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 220)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 220, 300)     39000000    input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_7 (SpatialDro (None, 220, 300)     0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 220, 256)     329472      spatial_dropout1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 220, 256)     329472      spatial_dropout1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 219, 64)      32832       bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 219, 64)      32832       bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_13 (Gl (None, 64)           0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 64)           0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 64)           0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 64)           0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           global_average_pooling1d_13[0][0]\n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_average_pooling1d_14[0][0]\n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 6)            1542        concatenate_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 39,726,150\n",
      "Trainable params: 726,150\n",
      "Non-trainable params: 39,000,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_12.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 779s 5ms/step - loss: 0.0535 - accuracy: 0.9807 - val_loss: 0.0410 - val_accuracy: 0.9842\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.988180 \n",
      "\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04103, saving model to best_model_12.hdf5\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 803s 6ms/step - loss: 0.0420 - accuracy: 0.9838 - val_loss: 0.0391 - val_accuracy: 0.9844\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.989352 \n",
      "\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04103 to 0.03911, saving model to best_model_12.hdf5\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 799s 6ms/step - loss: 0.0390 - accuracy: 0.9848 - val_loss: 0.0396 - val_accuracy: 0.9844\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.989320 \n",
      "\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03911\n",
      "153164/153164 [==============================] - 51s 334us/step\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 799s 6ms/step - loss: 0.0538 - accuracy: 0.9808 - val_loss: 0.0410 - val_accuracy: 0.9840\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.988301 \n",
      "\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.03911\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 792s 6ms/step - loss: 0.0420 - accuracy: 0.9838 - val_loss: 0.0408 - val_accuracy: 0.9843\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.989402 \n",
      "\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03911\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 796s 6ms/step - loss: 0.0394 - accuracy: 0.9846 - val_loss: 0.0394 - val_accuracy: 0.9842\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.989840 \n",
      "\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03911\n",
      "153164/153164 [==============================] - 49s 320us/step\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 805s 6ms/step - loss: 0.0546 - accuracy: 0.9804 - val_loss: 0.0422 - val_accuracy: 0.9833\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.988553 \n",
      "\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.03911\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 806s 6ms/step - loss: 0.0426 - accuracy: 0.9836 - val_loss: 0.0433 - val_accuracy: 0.9833\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.989441 \n",
      "\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03911\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 800s 6ms/step - loss: 0.0393 - accuracy: 0.9846 - val_loss: 0.0391 - val_accuracy: 0.9847\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.989961 \n",
      "\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03911\n",
      "153164/153164 [==============================] - 51s 332us/step\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 801s 6ms/step - loss: 0.0535 - accuracy: 0.9810 - val_loss: 0.0417 - val_accuracy: 0.9837\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.987746 \n",
      "\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.03911\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 808s 6ms/step - loss: 0.0421 - accuracy: 0.9838 - val_loss: 0.0405 - val_accuracy: 0.9847\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.988635 \n",
      "\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03911\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 807s 6ms/step - loss: 0.0392 - accuracy: 0.9847 - val_loss: 0.0398 - val_accuracy: 0.9846\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.989303 \n",
      "\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03911\n",
      "153164/153164 [==============================] - 52s 340us/step\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 1025s 7ms/step - loss: 0.0536 - accuracy: 0.9807 - val_loss: 0.0415 - val_accuracy: 0.9838\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.988009 \n",
      "\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.03911\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 1023s 7ms/step - loss: 0.0421 - accuracy: 0.9837 - val_loss: 0.0422 - val_accuracy: 0.9835\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.989774 \n",
      "\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03911\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 819s 6ms/step - loss: 0.0395 - accuracy: 0.9846 - val_loss: 0.0407 - val_accuracy: 0.9838\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.989807 \n",
      "\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03911\n",
      "153164/153164 [==============================] - 52s 341us/step\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 800s 6ms/step - loss: 0.0544 - accuracy: 0.9803 - val_loss: 0.0408 - val_accuracy: 0.9843\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.988451 \n",
      "\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.03911\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 797s 6ms/step - loss: 0.0425 - accuracy: 0.9836 - val_loss: 0.0413 - val_accuracy: 0.9839\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.989295 \n",
      "\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03911\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 799s 6ms/step - loss: 0.0396 - accuracy: 0.9846 - val_loss: 0.0397 - val_accuracy: 0.9843\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.989482 \n",
      "\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03911\n",
      "153164/153164 [==============================] - 52s 343us/step\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 791s 6ms/step - loss: 0.0547 - accuracy: 0.9803 - val_loss: 0.0409 - val_accuracy: 0.9840\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.987839 \n",
      "\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.03911\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 784s 5ms/step - loss: 0.0424 - accuracy: 0.9836 - val_loss: 0.0398 - val_accuracy: 0.9844\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.989322 \n",
      "\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03911\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 779s 5ms/step - loss: 0.0398 - accuracy: 0.9844 - val_loss: 0.0389 - val_accuracy: 0.9848\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.989740 \n",
      "\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03911 to 0.03895, saving model to best_model_12.hdf5\n",
      "153164/153164 [==============================] - 49s 320us/step\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 778s 5ms/step - loss: 0.0536 - accuracy: 0.9809 - val_loss: 0.0418 - val_accuracy: 0.9836\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.988974 \n",
      "\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.03895\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 774s 5ms/step - loss: 0.0423 - accuracy: 0.9837 - val_loss: 0.0416 - val_accuracy: 0.9837\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.989671 \n",
      "\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03895\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 770s 5ms/step - loss: 0.0395 - accuracy: 0.9846 - val_loss: 0.0409 - val_accuracy: 0.9837\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.989998 \n",
      "\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03895\n",
      "153164/153164 [==============================] - 46s 302us/step\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 771s 5ms/step - loss: 0.0544 - accuracy: 0.9806 - val_loss: 0.0424 - val_accuracy: 0.9837\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.988323 \n",
      "\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.03895\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 773s 5ms/step - loss: 0.0424 - accuracy: 0.9837 - val_loss: 0.0409 - val_accuracy: 0.9840\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.989477 \n",
      "\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03895\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 775s 5ms/step - loss: 0.0393 - accuracy: 0.9846 - val_loss: 0.0396 - val_accuracy: 0.9844\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.989969 \n",
      "\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03895\n",
      "153164/153164 [==============================] - 47s 308us/step\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 784s 5ms/step - loss: 0.0540 - accuracy: 0.9807 - val_loss: 0.0451 - val_accuracy: 0.9823\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.988619 \n",
      "\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.03895\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 780s 5ms/step - loss: 0.0422 - accuracy: 0.9838 - val_loss: 0.0410 - val_accuracy: 0.9845\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.989665 \n",
      "\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.03895\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143613/143613 [==============================] - 784s 5ms/step - loss: 0.0393 - accuracy: 0.9847 - val_loss: 0.0391 - val_accuracy: 0.9846\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.989843 \n",
      "\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.03895\n",
      "153164/153164 [==============================] - 47s 306us/step\n"
     ]
    }
   ],
   "source": [
    "pred_12 = 0\n",
    "n_seeds = 10\n",
    "\n",
    "for i in range(n_seeds):\n",
    "    model_12 = build_model_1(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2)\n",
    "    pred_12 += model_12.predict(test, batch_size = 1024, verbose = 1)/n_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[list_classes] = (pred_12)\n",
    "submission.to_csv(\"submission_12.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model_12,open('fasttextembedding.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_log = 'submission_test_lr_total.csv'\n",
    "dl_glove = 'submission_123.csv'\n",
    "dl_glove_50 = 'submission_1.csv'\n",
    "dl_fasttext = 'submission_12.csv'\n",
    "bl_sgdlog = 'submission_test_sgdlr.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_logr = pd.read_csv(bl_log)\n",
    "p_lstmgruglove = pd.read_csv(dl_glove)\n",
    "p_lstmgruglove50 = pd.read_csv(dl_glove_50)\n",
    "p_grufasttext= pd.read_csv(dl_fasttext)\n",
    "p_sgdlog = pd.read_csv(bl_sgdlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensemble model of glove and fastext embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "p_result = p_lstmgruglove.copy()\n",
    "p_result[class_names] = (0.33*p_grufasttext[class_names] + 0.67*p_lstmgruglove[class_names]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_result.to_csv('submission_ensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensemble model of logistic regression,glove and fasttext embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_result_1 = p_grufasttext.copy()\n",
    "p_result_1[class_names] = (0.3*p_logr[class_names] +0.3* p_lstmgruglove[class_names] + 0.4*p_grufasttext[class_names]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_result_1.to_csv('submission_ensemble_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensemble model of glove 50dim,glove 300dim and fasttext embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_result_2 = p_grufasttext.copy()\n",
    "p_result_2[class_names] = (0.4*p_lstmgruglove50[class_names] +0.3* p_lstmgruglove[class_names] + 0.3*p_grufasttext[class_names]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_result_2.to_csv('submission_ensemble_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** Baseline Models (ML) **************************************************\n",
      "\n",
      "+-------------------------------------------------------+----------------------+---------------------+\n",
      "|                         Model                         | kaggle private score | kaggle public score |\n",
      "+-------------------------------------------------------+----------------------+---------------------+\n",
      "|           logistic regression for test data           |       0.97712        |       0.97644       |\n",
      "|        logistic regression for total test data        |       0.97762        |       0.97691       |\n",
      "|               naive bayes for test data               |        0.9491        |       0.94918       |\n",
      "| logistic regression with SGD classifier for test data |       0.97684        |       0.97622       |\n",
      "+-------------------------------------------------------+----------------------+---------------------+\n",
      "************************************************** Nueral network Models (DL) **************************************************\n",
      "\n",
      "+------------------------------------------------+----------------------+---------------------+\n",
      "|                     Model                      | kaggle private score | kaggle public score |\n",
      "+------------------------------------------------+----------------------+---------------------+\n",
      "|  Bidirectional GRU with glove embedding 300d   |       0.98345        |       0.98407       |\n",
      "|   Bidirectional GRU with glove embedding 50d   |       0.98338        |       0.98453       |\n",
      "| Bidirectional GRU with fasttext embedding 300d |       0.98341        |       0.98436       |\n",
      "+------------------------------------------------+----------------------+---------------------+\n",
      "************************************************** Ensemble Models (DL) **************************************************\n",
      "\n",
      "+--------------------------------------------------------------------+----------------------+---------------------+\n",
      "|                               Model                                | kaggle private score | kaggle public score |\n",
      "+--------------------------------------------------------------------+----------------------+---------------------+\n",
      "|           ensemble model of glove and fasttext embedding           |       0.98407        |       0.98462       |\n",
      "| ensemble model of logistic regression,glove and fasttext embedding |        0.9848        |       0.98508       |\n",
      "| ensemble model of glove 50dim,glove 300dim and fasttext embedding  |       0.98428        |       0.98497       |\n",
      "+--------------------------------------------------------------------+----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "x=PrettyTable()\n",
    "x.field_names=[\"Model\",\"kaggle private score\",\"kaggle public score\"]\n",
    "x.add_row([\"logistic regression for test data\",0.97712,0.97644])\n",
    "x.add_row([\"logistic regression for total test data\",0.97762,0.97691])\n",
    "x.add_row([\"naive bayes for test data\",0.94910,0.94918])\n",
    "x.add_row([\"logistic regression with SGD classifier for test data\",0.97684,0.97622])\n",
    "\n",
    "\n",
    "x1=PrettyTable()\n",
    "x1.field_names=[\"Model\",\"kaggle private score\",\"kaggle public score\"]\n",
    "x1.add_row([\"Bidirectional GRU with glove embedding 300d\",0.98345,0.98407])\n",
    "x1.add_row([\"Bidirectional GRU with glove embedding 50d\",0.98338,0.98453])\n",
    "x1.add_row([\"Bidirectional GRU with fasttext embedding 300d\",0.98341,0.98436])\n",
    "\n",
    "x2=PrettyTable()\n",
    "x2.field_names=[\"Model\",\"kaggle private score\",\"kaggle public score\"]\n",
    "x2.add_row([\"ensemble model of glove and fasttext embedding\",0.98407,0.98462])\n",
    "x2.add_row([\"ensemble model of logistic regression,glove and fasttext embedding\",0.98480,0.98508])\n",
    "x2.add_row([\"ensemble model of glove 50dim,glove 300dim and fasttext embedding\",0.98428,0.98497])\n",
    "\n",
    "print(\"************************************************** Baseline Models (ML) **************************************************\\n\")\n",
    "print(x.get_string(start=0,end=9))\n",
    "print(\"************************************************** Nueral network Models (DL) **************************************************\\n\")\n",
    "print(x1.get_string(start=0,end=9))\n",
    "print(\"************************************************** Ensemble Models (DL) **************************************************\\n\")\n",
    "print(x2.get_string(start=0,end=9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
